# SFT Auto-Labeling Pipeline — Trial Summary & Analysis

> Date: 2026-02-24
> Author: Auto-generated by pipeline trial

## 1. Executive Summary

完成了 SFT 数据自动打标流水线的首轮测试。主要成果：

- **数据集**: 生成 108 条 SFT 样本（97 单轮 + 11 多轮 agentic），覆盖 44 个场景类别
- **打标流水线**: 双调用架构（Call 1: 定性 → Call 2: 定能力）+ 预处理 + 验证 + 仲裁
- **双模型测试**: deepseek-v3.2（中端）和 claude-sonnet-4-6（强模型）分别打标同一数据集
- **模型间一致率**: Intent 99%, Language 73%, Domain 62%, Task 53%, Difficulty 85%, Concept 32%, Agentic 53%, Constraint 67%, Context 70%

## 2. Pipeline Architecture

```
┌──────────────────────────────────────────────────────────────┐
│                    SFT Sample (ShareGPT)                     │
└─────────────────────────────┬────────────────────────────────┘
                              │
                    ┌─────────▼──────────┐
                    │   Preprocessing    │  ← local, ~0ms
                    │  - Code fence lang │
                    │  - Tool roles      │
                    │  - Keywords        │
                    │  - Turn structure  │
                    └─────────┬──────────┘
                              │
                    ┌─────────▼──────────┐
                    │     Call 1         │  ← LLM, ~5-30s
                    │  Intent, Language  │
                    │  Domain, Task      │
                    │  Difficulty        │
                    └─────────┬──────────┘
                              │
                    ┌─────────▼──────────┐
                    │     Call 2         │  ← LLM, ~5-30s
                    │  Concept, Agentic  │     (receives Call 1 results)
                    │  Constraint        │
                    │  Context           │
                    └─────────┬──────────┘
                              │
              ┌───────────────▼───────────────┐
              │      Validation & Merge       │  ← local
              │  - Tag pool check             │
              │  - Unmapped tag collection     │
              │  - Consistency rules           │
              └───────────────┬───────────────┘
                              │
               low conf?  ┌───▼───┐  high conf
              ┌───────────│ Check │─────────────┐
              │           └───────┘             │
    ┌─────────▼──────────┐              ┌──────▼──────┐
    │    Arbitration     │              │   Output    │
    │  Re-call with      │              │  labeled    │
    │  temp=0.3          │              │  sample     │
    └─────────┬──────────┘              └─────────────┘
              │
              └──────────────────────────────────────→ Output
```

## 3. Test Results

### 3.1 deepseek-v3.2 (Mid-tier)

| Metric | v1 (97) | v2 (108) | v3 (108) | v4 (108) |
|--------|---------|----------|----------|----------|
| Success rate | 100% | 100% | 100% | 100% |
| Avg calls/sample | 2.0 | 2.0 | 2.0 | 2.0 |
| Arbitration rate | 0% | 0% | 0% | 0% |
| Total tokens | 1.01M | 1.22M | 1.26M | 1.30M |
| Elapsed | 527s | 614s | 764s | 522s |
| Unmapped tags | 3 | 5 | 3 | 3 |

### 3.2 claude-sonnet-4-6 (Strong)

| Metric | v1 (97) | v2 (108) | v3 (108) | v4 (108) |
|--------|---------|----------|----------|----------|
| Success rate | 100% | 100% | 100% | 100% |
| Avg calls/sample | 2.1 | 2.0 | 2.0 | 2.0 |
| Arbitration rate | 10.3% | 4.6% | 3.7% | 2.8% |
| Total tokens | 1.25M | 1.46M | 1.50M | 1.54M |
| Elapsed | 215s | 34s | 41s | 357s |
| Unmapped tags | 79 | 67 | 56 | 33 |

## 4. Model Comparison Analysis

### 4.1 v1 → v2 Agreement Improvement

| Dimension | v1 Exact | v2 Exact | Change | v2 Partial | v2 Jaccard |
|-----------|----------|----------|--------|------------|------------|
| **Intent** | 99.0% | 99.1% | +0.1% | 99.1% | 0.991 |
| **Language** | 73.2% | 72.2% | -1.0% | 99.1% | 0.880 |
| **Domain** | 61.9% | 67.6% | +5.7% | 94.4% | 0.809 |
| **Task** | 52.6% | 46.3% | -6.3% | 97.2% | 0.724 |
| **Difficulty** | 84.5% | 82.4% | -2.1% | 82.4% | 0.824 |
| **Concept** | 32.0% | 33.3% | +1.3% | 92.6% | 0.636 |
| **Agentic** | 52.6% | **89.8%** | **+37.2%** | 97.2% | 0.954 |
| **Constraint** | 67.0% | 63.0% | -4.0% | 73.2% | 0.669 |
| **Context** | 70.1% | 69.4% | -0.7% | 69.4% | 0.694 |

**v2 优化亮点:**
- **Agentic 维度大幅提升** (52.6% → 89.8%)：prompt 中强化了 "behavioral patterns only for actual agent behavior" 后，Sonnet 不再对纯 Q&A 对话标注 multi-step-reasoning
- **Sonnet 仲裁率下降** (10.3% → 4.6%)：更清晰的指导降低了模型的不确定性
- **Sonnet unmapped 减少** (79 → 67)：部分改善但仍需进一步优化
- **Dataset 扩展**: 108 样本（v1: 97 样本），加入 11 条 multi-turn agentic

### 4.2 Key Divergence Patterns (v2)

**Pattern 1: Concept — data-structures 过度标注（最大分歧源）**
- Deepseek `data-structures` 使用 39 次 vs Sonnet 18 次 (差 21 次)
- Deepseek 将 data-structures 作为 catch-all：只要代码涉及 list/dict 就标注
- Sonnet 仅在数据结构选型是核心话题时才标注
- 根本问题：标注阈值不同 — Deepseek "bottom-up"(有什么数据)，Sonnet "top-down"(用什么模式)

**Pattern 2: Concept — design-patterns 边界模糊**
- Sonnet `design-patterns` 使用 27 次 vs Deepseek 8 次 (差 19 次)
- 10 条样本中出现 `data-structures(DS) ↔ design-patterns(SN)` 替换
- 示例：LSP 服务端实现 — DS 标 data-structures(JSON消息), SN 标 design-patterns(OOP架构)
- 说明模型对同一证据的 conceptual framing 不同

**Pattern 3: Task — code-optimization 过度标注**
- Deepseek `code-optimization` 使用 20 次 vs Sonnet 6 次 (差 14 次)
- Deepseek 将 "配置" 或 "最佳实践" 问题归为优化
- Sonnet 更倾向标 feature-implementation, error-handling-task

**Pattern 4: Sonnet unmapped tags 仍多 (67 个)**
- 从 v1 的 79 降至 67，但仍远高于 Deepseek 的 5 个
- Sonnet 仍然创造特定领域标签如 `distributed-systems`, `CRDT`, `gpu-programming`
- 需要更强的 pool 约束或后处理映射

## 5. Model Selection Recommendation

### Production Labeling: deepseek-v3.2

**理由：**
1. **Tag pool 合规性**: 仅 3 个 unmapped tags (vs 79)，说明对指令的遵循更好
2. **置信度一致性**: 全部 0% 仲裁率，输出稳定
3. **成本效益**: 在生产场景（200万+数据）下，token 用量更低
4. **可预测性**: 输出格式稳定，不需要额外的 unmapped tag 处理

### Gold Set Annotation: claude-sonnet-4-6 (or stronger)

**理由：**
1. **更准确的 Difficulty 判断**: Sonnet 对高级概念的难度评估更合理
2. **更全面的 Domain 覆盖**: 识别出更多相关域
3. **仲裁能力**: 低置信度时能触发仲裁重标，更诚实

### Hybrid Strategy (推荐)

```
Production pipeline:
  1. deepseek-v3.2 (Call 1 + Call 2)      ← 主力标注
  2. If conf < 0.65: sonnet-4-6 arbitrate  ← 仅仲裁低置信度
  3. If unmapped > 0: sonnet-4-6 map       ← 语义映射

Gold set construction:
  1. 3 strong models independently label   ← opus-4.5, gpt-5, gemini-2.5-pro
  2. Agreement → auto-accept
  3. Disagreement → human review
```

## 6. Prompt Optimization History

### 6.1 v2 优化（已实施，效果验证）

| 目标 | 方法 | v1→v2 结果 |
|------|------|-----------|
| Agentic | 强化 "behavioral patterns = actual agent behavior" + negative examples | **52.6% → 89.8%** ✅ |
| Concept | 添加 mapping examples + "NEVER invent new tags" | 32.0% → 33.3% (微升) |
| Difficulty | 添加 beginner~expert 具体案例 | 84.5% → 82.4% (持平) |
| Context | 添加 scope hierarchy + 边界示例 | 70.1% → 69.4% (持平) |

**结论**: Agentic 优化非常成功。Concept 需要更深层的策略性修改。

### 6.2 v3 优化（已实施，效果验证）

| 目标 | 方法 | v2→v3 结果 |
|------|------|-----------|
| Concept: data-structures catch-all | 添加 THRESHOLD 规则 "only when choice/design is meaningful focus" | DS 39→27, SN 18→12, **33.3%→39.8%** (+6.5%) ✅ |
| Concept: design-patterns boundary | 收窄为 "named patterns only" + negative examples | DS 8→2, SN 27→10, gap 19→8 ✅ |
| Concept: error-handling threshold | 添加 "non-trivial focus only" | SN 30→20, 减少 boilerplate tagging ✅ |
| Task: code-optimization overuse | 添加 "ONLY for explicit performance improvement" | DS 20→11 (-9), **46.3%→54.6%** (+8.3%) ✅ |

**v3 Concept/Task tag gap convergence:**

```
                     v2 gap    v3 gap    Improvement
data-structures:     21        15        -29%
design-patterns:     19         8        -58%
error-handling:      10         6        -40%
code-optimization:   14         6        -57%
algorithms:           7         4        -43%
```

### 6.3 v4 优化（已实施，效果验证）

基于 13 条 Concept 分歧样本的**人工审核**结论，v4 做了以下改进：

| 目标 | 方法 | v3→v4 结果 |
|------|------|-----------|
| Concept: 空值合法性 | 添加 "Empty is a valid answer" 规则 | DS 空值增多，减少低质量标注 ✅ |
| Concept: 框架≠概念 | 添加 "Framework-specific knowledge is NOT a concept" 规则 | 减少 React hooks/Vue lifecycle 误标 ✅ |
| Concept: 正向信号 | 添加 Positive signals 映射表（decorators→metaprogramming, TTL→caching 等） | **39.8%→50.9%** (+11.1%) ✅ |
| Sonnet unmapped | 加强 pool 约束 + 更多 mapping examples | 56→33 (-41%) ✅ |

### 6.4 Cumulative Prompt Optimization Results (v1→v2→v3→v4)

| Dimension | v1 Exact | v2 Exact | v3 Exact | v4 Exact | Total Δ |
|-----------|----------|----------|----------|----------|---------|
| **Intent** | 99.0% | 99.1% | 99.1% | 99.1% | +0.1% |
| **Language** | 73.2% | 72.2% | 73.2% | 69.4% | -3.8% |
| **Domain** | 61.9% | 67.6% | 65.7% | 63.0% | +1.1% |
| **Task** | 52.6% | 46.3% | 54.6% | **60.2%** | **+7.6%** |
| **Difficulty** | 84.5% | 82.4% | 81.5% | **84.3%** | -0.2% |
| **Concept** | 32.0% | 33.3% | 39.8% | **50.9%** | **+18.9%** |
| **Agentic** | 52.6% | **89.8%** | 87.0% | 88.0% | **+35.4%** |
| **Constraint** | 67.0% | 63.0% | 66.7% | 68.5% | +1.5% |
| **Context** | 70.1% | 69.4% | 73.2% | 71.3% | +1.2% |
| **Overall** | **65.9%** | **69.2%** | **71.2%** | **72.7%** | **+6.8%** |

**v4 Concept tag gap convergence (v3→v4):**

```
                     v2 gap    v3 gap    v4 gap    v3→v4
data-structures:     21        15         5        -67%
design-patterns:     19         8         4        -50%
error-handling:      10         6         5        -17%
algorithms:           7         4         2        -50%
concurrency:          -         -         6        (new tracking)
```

**结论**: 四轮优化将整体一致率从 65.9% 提升到 72.7%。v4 的核心突破是 **Concept 维度从 39.8% 跃升到 50.9% (+11.1%)**，这得益于基于人工审核的精准规则设计。最大累计改善：Agentic (+35.4%), Concept (+18.9%), Task (+7.6%)。

### 6.5 Remaining Divergence Analysis (v4)

v4 后的主要分歧源：

**Concept (50.9% exact, 90.7% partial, Jaccard 0.706)**
- 已从 32.0% 提升至 50.9%，改善显著
- Partial match 90.7% 说明模型大方向高度一致
- 剩余分歧主要来自：Sonnet 标注更多补充概念（如 memory-management 额外搭配 ownership）
- 可接受 50-55% 作为 multi-label 主观维度的自然上限

**Task (60.2% exact, 98.2% partial, Jaccard 0.792)**
- feature-implementation gap: DS 47, SN 59 (Sonnet 更倾向标此)
- 部分分歧来自 configuration vs feature-implementation 的边界

**Language (69.4% exact, 100% partial, Jaccard 0.868)**
- Sonnet 识别更多辅助语言（shell, html 等配置文件语言）
- Partial match 100% 说明主要语言完全一致，仅在辅助语言数量上有差异

**Unmapped 趋势:**
```
          v1    v2    v3    v4
DS:       3     5     3     3    (稳定)
SN:       79    67    56    33   (持续改善 ↓58%)
```

## 7. Pipeline Performance

### 7.1 吞吐量 (v4)

| Model | Concurrency | Samples | Time | Throughput |
|-------|-------------|---------|------|------------|
| deepseek-v3.2 | 30 | 108 | 522s | 0.21/s |
| claude-sonnet-4-6 | 30 | 108 | 357s | 0.30/s |

### 7.2 预估生产规模成本

假设 200 万样本：
- **deepseek-v3.2**: ~1M tok/97样本 → ~20.6B tokens → ~$2K-5K (按 $0.1-0.25/M token)
- **claude-sonnet-4-6**: ~1.25M tok/97样本 → ~25.8B tokens → ~$50K-75K (按 $3/M token)
- **Hybrid**: deepseek 主力 + 10% 仲裁用 sonnet → ~$5K-10K

### 7.3 预估生产时间

- deepseek-v3.2 @ concurrency=50: ~200万 / (0.18×50/30) = ~6.7M 秒 → ~77 天 → 需要更高并发
- claude-sonnet-4-6 @ concurrency=50: ~200万 / (0.45×50/30) = ~2.7M 秒 → ~31 天
- **建议**: concurrency=100 + 多 worker 实例 → 可缩短到数天

## 8. Dataset Summary

### 8.1 Generated Samples

| Category | Count | Notes |
|----------|-------|-------|
| Total | 108 | |
| Single-turn | 97 | Standard Q&A |
| Multi-turn (agentic) | 11 | With tool roles |
| Categories covered | 44 | All 5 intents × multiple difficulties |

### 8.2 Coverage by Intent

| Intent | Count | Difficulties covered |
|--------|-------|---------------------|
| learn | 22 | beginner(8), intermediate(8), advanced(5), expert(1) |
| build | 48 | beginner(7), intermediate(24), advanced(12), expert(5) |
| debug | 12 | beginner(3), intermediate(6), advanced(3) |
| review | 6 | beginner(1), intermediate(3), advanced(2) |
| decide | 9 | beginner(2), intermediate(6), advanced(1) |

### 8.3 Files Produced

```
data/gold_set/
├── raw_samples.json              # 108 generated SFT samples
├── labeled_deepseek.json         # v1 labels by deepseek-v3.2 (97 samples)
├── labeled_sonnet.json           # v1 labels by claude-sonnet-4-6 (97 samples)
├── labeled_deepseek_v2.json      # v2 labels by deepseek-v3.2 (108 samples)
├── labeled_sonnet_v2.json        # v2 labels by claude-sonnet-4-6 (108 samples)
├── labeled_deepseek_v3.json      # v3 labels by deepseek-v3.2 (108 samples)
├── labeled_sonnet_v3.json        # v3 labels by claude-sonnet-4-6 (108 samples)
├── labeled_deepseek_v4.json      # v4 labels by deepseek-v3.2 (108 samples)
├── labeled_sonnet_v4.json        # v4 labels by claude-sonnet-4-6 (108 samples)
├── labeled_samples.json          # Default (copy of deepseek v1)
├── labeling_stats.json           # Stats for deepseek v1
├── stats_*.json                  # Stats per model per version
├── model_comparison_report.md    # v1 comparison
├── model_comparison_v2.md        # v2 comparison
├── model_comparison_v3.md        # v3 comparison
├── model_comparison_v4.md        # v4 comparison (current best)
├── labeling_report.md            # Detailed single-model report
├── labeling_monitor.jsonl        # Monitor log (deepseek v1)
└── monitor_*.jsonl               # Monitor logs per run
```

## 9. Next Steps

### Immediate (Phase 1) — Completed ✅
- [x] **v2 Prompt 优化**: Agentic, Context, Difficulty, Concept
- [x] **v2 验证**: Agentic 52.6%→89.8% ✅
- [x] **v3 Prompt 优化**: Concept (data-structures threshold), Task (code-optimization threshold)
- [x] **v3 验证**: Concept 33.3%→39.8% ✅, Task 46.3%→54.6% ✅, Overall 69.2%→71.2% ✅
- [x] **v4 人工审核**: 13 条 Concept 分歧样本，判定哪个模型更准确
- [x] **v4 Prompt 优化**: "Empty is valid" + "Framework≠Concept" + Positive signals
- [x] **v4 验证**: Concept 39.8%→50.9% ✅, Task 54.6%→60.2% ✅, Overall 71.2%→72.7% ✅

### Short-term (Phase 2) — Next
- [ ] **Gold set 人工审核**: 抽取 20-30 条样本，人工确定 Concept/Context 维度 ground truth
- [ ] **扩大数据集**: 生成更多 multi-turn agentic 数据（当前仅 11 条）
- [ ] **更多模型测试**: gpt-4o-mini, qwen3-235b 等候选模型
- [ ] **Confidence 阈值调优**: 基于人工审核调整仲裁阈值

### Production (Phase 3)
- [ ] **批量处理框架**: 支持断点续传、进度监控、错误恢复
- [ ] **Monitoring Dashboard**: 实时监控打标进度、质量指标
- [ ] **Taxonomy Evolution**: unmapped tag 审查 → 标签池迭代

---
_Generated by SFT Auto-Labeling Pipeline Trial, 2026-02-24_
